{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maguid28/EE6041-TAandNLP/blob/main/Etivity1_2_23222425.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etivity 1.2"
      ],
      "metadata": {
        "id": "MMlxmD-jyURk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qouNyB8pyRIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code"
      ],
      "metadata": {
        "id": "jTQ0PUzty6b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "\n",
        "!wget https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "filePath = \"/content/100-0.txt\"\n",
        "\n",
        "ShakespeareFile = open(filePath, \"r\")\n",
        "ShakespeareContent = ShakespeareFile.read()\n",
        "\n",
        "# splitlines splits a string into a list. The splitting is at line breaks\n",
        "ShakespeareContent = ShakespeareContent.splitlines()\n",
        "\n",
        "print(ShakespeareContent[0:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQrS5Jdh0w0J",
        "outputId": "ece8622e-3b1d-465d-dd48-ff29425e7d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 17:26:06--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5647299 (5.4M) [text/plain]\n",
            "Saving to: ‘100-0.txt.4’\n",
            "\n",
            "100-0.txt.4         100%[===================>]   5.38M  3.08MB/s    in 1.7s    \n",
            "\n",
            "2023-09-29 17:26:09 (3.08 MB/s) - ‘100-0.txt.4’ saved [5647299/5647299]\n",
            "\n",
            "['\\ufeffThe Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare', '', 'This eBook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this eBook or online at', 'www.gutenberg.org. If you are not located in the United States, you', 'will have to check the laws of the country where you are located before', 'using this eBook.', '', 'Title: The Complete Works of William Shakespeare', '', 'Author: William Shakespeare', '', 'Release Date: January 1994 [eBook #100]', '[Most recently updated: September 24, 2023]', '', 'Language: English', '', '', '*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***', '', '', '', '', 'The Complete Works of William Shakespeare', '', 'by William Shakespeare', '', '', '', '', '                    Contents', '', '    THE SONNETS', '    ALL’S WELL THAT ENDS WELL', '    THE TRAGEDY OF ANTONY AND CLEOPATRA', '    AS YOU LIKE IT', '    THE COMEDY OF ERRORS', '    THE TRAGEDY OF CORIOLANUS', '    CYMBELINE', '    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK', '    THE FIRST PART OF KING HENRY THE FOURTH', '    THE SECOND PART OF KING HENRY THE FOURTH', '    THE LIFE OF KING HENRY THE FIFTH', '    THE FIRST PART OF HENRY THE SIXTH', '    THE SECOND PART OF KING HENRY THE SIXTH', '    THE THIRD PART OF KING HENRY THE SIXTH', '    KING HENRY THE EIGHTH', '    THE LIFE AND DEATH OF KING JOHN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Use the tf.keras.preprocessing.text.Tokenizer class\n",
        "to:\n",
        "\n",
        "  a. Tokenize the Shakespeare corpus\n",
        "\n",
        "  b. Print out the total number of  Tokens in the corpus\n",
        "\n",
        "  c. print out the total number of Types in the corpus\n",
        "\n",
        "  d. Print out the most frequent Type in corpus and its frequency\n",
        "\n"
      ],
      "metadata": {
        "id": "G2gqr_-o2m8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    analyzer=None\n",
        ")"
      ],
      "metadata": {
        "id": "B-VLnXJM2mBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the Shakespeare corpus\n",
        "tokenizer.fit_on_texts(ShakespeareContent)\n",
        "\n",
        "num_tokens = sum(tokenizer.word_counts.values())\n",
        "print(\"Total number of tokens: \" + str(num_tokens))\n",
        "\n",
        "num_types = len(tokenizer.word_counts)\n",
        "print(\"Total number of types: \" + str(num_types))\n",
        "\n",
        "most_frequent_type = max(tokenizer.word_counts, key=tokenizer.word_counts.get)\n",
        "print(\"Most frequent Type:\" + str(most_frequent_type))\n",
        "print(\"with frequency: \" + str(tokenizer.word_counts[most_frequent_type]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LW24I5C3l9G",
        "outputId": "73d38798-431a-421f-eff7-8962103e0af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 972136\n",
            "Total number of types: 30491\n",
            "Most frequent Type:the\n",
            "with frequency: 30449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Use the PorterStemmer  and WordNetLemmatizer classes in the NLTK.stem package\n",
        "\n",
        "to:\n",
        "\n",
        "a).stem all the Types in the Shakespeare corpus; print out the total number of Types in the corpus after stemming.\n",
        "\n",
        "b). Lemmatize all the Types in the Shakespeare corpus; print out the total number of Types in the corpus after lemmatization.\n",
        "\n",
        "c). Assert the validity of this expression:  total_number_of_types > total_number_of_lemmatized_types > total_number_of_stemmed_types\n"
      ],
      "metadata": {
        "id": "6S7g1Vw97-Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROJ7XB6d8PIp",
        "outputId": "cc4f75b3-6053-4adf-c589-47910d51ecc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stem all the types in Shakespeare corpus\n",
        "# Store in set to insure uniqueness\n",
        "stemmed_types = set([ps.stem(word) for word in tokenizer.word_counts.keys()])\n",
        "print(\"Total number of types after stemming: \" + str(len(stemmed_types)))\n",
        "\n",
        "# Lematize all the types in Shakespeare corpus\n",
        "lemmatized_types = set([lemmatizer.lemmatize(word) for word in tokenizer.word_counts.keys()])\n",
        "print(\"Total number of types after lemmatization: \" + str(len(lemmatized_types)))\n",
        "\n",
        "# Assert validity\n",
        "assert num_types > len(lemmatized_types) > len(stemmed_types), \"Assertion failed!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXpho5al9I-9",
        "outputId": "a7b3f91d-7b8c-4ab7-8960-a47590b6959d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of types after stemming: 21336\n",
            "Total number of types after lemmatization: 27188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Use the Sentence Segmentation module in the spaCy package\n",
        "to:\n",
        "\n",
        "Segment the last 100 lines of the Shakespeare corpus into sentences\n",
        "print out the segmented sentences and their total number.\n"
      ],
      "metadata": {
        "id": "fs3KZMXMEhSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "last_100_lines = ShakespeareContent[-100:]\n",
        "\n",
        "last_100_lines_combined = ' '.join(last_100_lines)\n",
        "\n",
        "doc = nlp(last_100_lines_combined)\n",
        "\n",
        "sentences = []\n",
        "for sentence in doc.sents:\n",
        "    print(sentence.text)\n",
        "    sentences.append(sentence.text)\n",
        "\n",
        "print(\"Total num of sentences: \" + str(len(sentences)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VF6Bj5lEpGW",
        "outputId": "4bbbb75c-b2a8-4bf6-9759-3b7a30842a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "production, promotion and distribution of Project Gutenberg™ electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg™ work, (b) alteration, modification, or additions or deletions to any Project Gutenberg™ work, and (c) any Defect you cause.  \n",
            "Section 2.\n",
            "Information about the Mission of Project Gutenberg™  Project Gutenberg™ is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers.\n",
            "It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life.  \n",
            "Volunteers and financial support to provide volunteers with the assistance they need are critical to reaching Project Gutenberg™'s goals and ensuring that the Project Gutenberg™ collection will remain freely available for generations to come.\n",
            "In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg™ and future generations.\n",
            "To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation information page at www.gutenberg.org.  Section 3.\n",
            "Information about the Project Gutenberg Literary Archive Foundation  The Project Gutenberg Literary Archive Foundation is a non-profit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the Internal Revenue Service.\n",
            "The Foundation’s EIN or federal tax identification number is 64-6221541.\n",
            "Contributions to the Project Gutenberg Literary Archive Foundation are tax deductible to the full extent permitted by U.S. federal laws and your state's laws.  \n",
            "The Foundation’s business office is located at 809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887.\n",
            "Email contact links and up to date contact information can be found at the Foundation’s website and official page at www.gutenberg.org/contact.  \n",
            "Section 4. Information about Donations to the Project Gutenberg Literary Archive Foundation  Project Gutenberg™ depends upon and cannot survive without widespread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine-readable form accessible by the widest array of equipment including outdated equipment.\n",
            "Many small donations ($1 to $5,000) are particularly important to maintaining tax exempt status with the IRS.  \n",
            "The Foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the United States.\n",
            "Compliance requirements are not uniform and it takes a considerable effort, much paperwork and many fees to meet and keep up with these requirements.\n",
            "We do not solicit donations in locations where we have not received written confirmation of compliance.\n",
            "To SEND DONATIONS or determine the status of compliance for any particular state visit www.gutenberg.org/donate.  \n",
            "While we cannot and do not solicit contributions from states where we have not met the solicitation requirements, we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate.  \n",
            "International donations are gratefully accepted, but we cannot make any statements concerning tax treatment of donations received from outside the United States.\n",
            "U.S. laws alone swamp our small staff.  \n",
            "Please check the Project Gutenberg web pages for current donation methods and addresses.\n",
            "Donations are accepted in a number of other ways including checks, online payments and credit card donations.\n",
            "To donate, please visit: www.gutenberg.org/donate.  \n",
            "Section 5.\n",
            "General Information About Project Gutenberg™ electronic works  Professor Michael S. Hart was the originator of the Project Gutenberg™ concept of a library of electronic works that could be freely shared with anyone.\n",
            "For forty years, he produced and distributed Project Gutenberg™ eBooks with only a loose network of volunteer support.  \n",
            "Project Gutenberg™ eBooks are often created from several printed editions, all of which are confirmed as not protected by copyright in the U.S. unless a copyright notice is included.\n",
            "Thus, we do not necessarily keep eBooks in compliance with any particular paper edition.  \n",
            "Most people start at our website which has the main PG search facility: www.gutenberg.org.  \n",
            "This website includes information about Project Gutenberg™, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks.  \n",
            "Total num of sentences: 31\n"
          ]
        }
      ]
    }
  ]
}
