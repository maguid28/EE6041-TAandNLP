{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHSD8LaW5H+YZ+RWEi2Y/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maguid28/EE6041-TAandNLP/blob/main/Etivity_6_1_23222425.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IwSyPSLLPufK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LHqzv83cP1Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Given the two confusion tables below, compute\n",
        "Microaveraged precision, recall, and F1\n",
        "Macroaveraged precision, recall, and F1\n",
        "Explain the reason for the difference between the obtained Microaveraged and Macroaveraged F1 measures.\n"
      ],
      "metadata": {
        "id": "aJgwf5hdP5Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## a). Microaveraged precision, recall, and F1\n",
        "\n",
        "$$ \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{870}{870 + 230} = \\frac{870}{1100} = 0.791$$\n",
        "\n",
        "\n",
        "--------------------------------------------------------------\n",
        "$$ \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{870}{870 + 230} = 0.791 $$\n",
        "\n",
        "\n",
        "\n",
        "--------------------------------------------------------------\n",
        "\n",
        "$$ \\text{F1 Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} =  \\frac{2 \\times \\text{0.7909} \\times \\text{0.7909}}{\\text{0.7909} + \\text{0.7909}} = \\frac{1.2510}{1.5818} = 0.791$$"
      ],
      "metadata": {
        "id": "9UOMaWypclPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## b). Macroaveraged precision, recall, and F1\n",
        "\n",
        "\n",
        "\n",
        "Class: Food\n",
        "  $$ \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{800}{800 + 200}  = \\frac{800}{1000} = 0.8$$\n",
        "\n",
        "Class: Drink\n",
        "  $$ \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{70}{70 + 30}  = \\frac{70}{100} = 0.7$$\n",
        "\n",
        "Macro Avg:\n",
        "  $$ \\text{Macro average Precision} = \\frac{0.8 + 0.7}{2} = 0.75$$\n",
        "\n",
        "\n",
        "--------------------------------------------------------------\n",
        "\n",
        "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "Class: Food\n",
        "  $$ \\text{Recall} = \\frac{800}{800 + 200} = \\frac{800}{1000} = 0.8$$\n",
        "\n",
        "Class: Drink\n",
        "  $$ \\text{Recall} = \\frac{70}{70 + 30} = \\frac{70}{70 + 30} = 0.7$$\n",
        "\n",
        "Macro Avg:\n",
        "  $$ \\text{Macro average Recall} = \\frac{0.8 + 0.7}{2} = 0.75$$\n",
        "\n",
        "--------------------------------------------------------------\n",
        "\n",
        "$$ \\text{F1 Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}  $$\n",
        "\n",
        "  $$ \\text{F1 Score} = \\frac{2 \\times \\text{0.75} \\times \\text{0.75}}{\\text{0.75} + \\text{0.75}} =  \\frac{1.125}{1.5} = 0.75$$\n"
      ],
      "metadata": {
        "id": "NFdzfHgverGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C). Explain the reason for the difference between the obtained Microaveraged and Macroaveraged F1 measures\n",
        "The difference in the micro and macro averaging is because for Macroaveraging we compute the each class individually and then average, whereas for microaveraging we take the two classes, add them together to get the micro averaged table.\n",
        "Macroaveraging has the benefit of giving classes with less values a stronger representation.\n",
        "In microaveraging if one classes values are too small then that classes impact on the final average can be minimal."
      ],
      "metadata": {
        "id": "uK84H9_IsxTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Modify the Multinomial NaÃ¯ve Bayes classifier function that you developed in Etivity5, Task3 to train and test a sentiment classifier"
      ],
      "metadata": {
        "id": "N4jmWUVjiaxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainingSet = [('just plain boring','-'),('entirely predictable and lacks energy','-'),('no surprises and very few laughs','-'),('very powerful','+'),('the most fun film of the summer','+')]\n",
        "testSet = [('predictable with no fun','?')]"
      ],
      "metadata": {
        "id": "7GonxDnmkS-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_prior_probabilities(dataset):\n",
        "    total_samples = len(dataset)\n",
        "    #print(f\"total samples:  {total_samples}\")\n",
        "    #print(dataset)\n",
        "    neg_count = 0\n",
        "    pos_count = 0\n",
        "\n",
        "    for _, label in dataset :\n",
        "      if label == '-':\n",
        "        neg_count+=1\n",
        "      else :\n",
        "        pos_count+=1\n",
        "\n",
        "    #print(neg_count)\n",
        "    #print(dataset)\n",
        "\n",
        "    prior_probability_neg = neg_count / total_samples\n",
        "    prior_probability_pos = pos_count / total_samples\n",
        "\n",
        "    return prior_probability_neg, prior_probability_pos"
      ],
      "metadata": {
        "id": "9u-c4Bq7j0EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priorPOS, priorNEG = calculate_prior_probabilities(trainingSet)"
      ],
      "metadata": {
        "id": "nnXZM1NUj3l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bow(dataset, min_frequency = 1):\n",
        "    all_words = []\n",
        "    for sample, _ in dataset:\n",
        "        all_words.extend(sample.lower().split())\n",
        "\n",
        "    word_frequencies = {}\n",
        "    for word in all_words:\n",
        "        if word in word_frequencies:\n",
        "            word_frequencies[word] += 1\n",
        "        else:\n",
        "            word_frequencies[word] = 1\n",
        "\n",
        "    bag_of_words = {word: count for word, count in word_frequencies.items() if count >= min_frequency}\n",
        "    return bag_of_words"
      ],
      "metadata": {
        "id": "MeCsry0KltU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words= create_bow(trainingSet)"
      ],
      "metadata": {
        "id": "VhLEwsiFj6_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_likelihoods(dataset, bag_of_words):\n",
        "    laplace_smoothing = 1\n",
        "    pos_samples = [text.lower() for text, label in dataset if label == 1]\n",
        "    neg_samples = [text.lower() for text, label in dataset if label == 0]\n",
        "\n",
        "    pos_word_counts = {word: laplace_smoothing for word in bag_of_words}\n",
        "    neg_word_counts = {word: laplace_smoothing for word in bag_of_words}\n",
        "\n",
        "    for text in pos_samples:\n",
        "        for word in text.split():\n",
        "            if word in pos_word_counts:\n",
        "                pos_word_counts[word] += 1\n",
        "\n",
        "    for text in neg_samples:\n",
        "        for word in text.split():\n",
        "            if word in neg_word_counts:\n",
        "                neg_word_counts[word] += 1\n",
        "\n",
        "    pos_total_count = sum(pos_word_counts.values())\n",
        "    neg_total_count = sum(neg_word_counts.values())\n",
        "\n",
        "    pos_likelihoods = {word: pos_word_counts[word] / pos_total_count for word in pos_word_counts}\n",
        "    neg_likelihoods = {word: neg_word_counts[word] / neg_total_count for word in neg_word_counts}\n",
        "\n",
        "    return pos_likelihoods, neg_likelihoods"
      ],
      "metadata": {
        "id": "0IzR6e1bj_Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def posterior_probability(new_sample, prior_probabilities, likelihoods):\n",
        "    neg_likelihoods, pos_likelihoods = likelihoods\n",
        "    prior_probability_neg, prior_probability_pos = prior_probabilities\n",
        "\n",
        "    words = new_sample.lower().split()\n",
        "\n",
        "    log_pos_prob = np.log(prior_probability_pos)\n",
        "    log_neg_prob = np.log(prior_probability_neg)\n",
        "\n",
        "    for word in words:\n",
        "        pos_likelihood = pos_likelihoods.get(word, 1 / (sum(pos_likelihoods.values()) + len(pos_likelihoods)))\n",
        "        ie_likelihood = neg_likelihoods.get(word, 1 / (sum(neg_likelihoods.values()) + len(neg_likelihoods)))\n",
        "\n",
        "        print(f\"Word: {word} \\t wordConditionalProbPos: {pos_likelihood} \\t wordConditionalProbNeg: {ie_likelihood}\")\n",
        "\n",
        "        log_pos_prob += np.log(pos_likelihood)\n",
        "        log_neg_prob += np.log(ie_likelihood)\n",
        "\n",
        "    #Convert back to normal probabilities\n",
        "    pos_prob = np.exp(log_pos_prob)\n",
        "    neg_prob = np.exp(log_neg_prob)\n",
        "\n",
        "    total_prob = pos_prob + neg_prob\n",
        "    posterior_probability_pos = pos_prob / total_prob\n",
        "    posterior_probability_neg = neg_prob / total_prob\n",
        "\n",
        "    return posterior_probability_pos, posterior_probability_neg"
      ],
      "metadata": {
        "id": "DwGzXYWkkAwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_likelihoods, POS_likelihoods = calculate_likelihoods(trainingSet, bag_of_words)\n",
        "likelihoods = (NEG_likelihoods, POS_likelihoods)"
      ],
      "metadata": {
        "id": "ffHj9Gz_kCqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(sentence, prior_probabilities, likelihoods):\n",
        "    posterior_probability_pos, posterior_probability_neg = posterior_probability(sentence, prior_probabilities, likelihoods)\n",
        "    print(f\"\\ndocProbPos: {posterior_probability_pos}\")\n",
        "    print(f\"docProbNeg: {posterior_probability_neg}\")\n",
        "\n",
        "    if posterior_probability_pos > posterior_probability_neg:\n",
        "        classification = \"Positive\"\n",
        "    elif posterior_probability_neg > posterior_probability_pos:\n",
        "        classification = \"Negative\"\n",
        "    else:\n",
        "        classification = \"Neutral\"\n",
        "\n",
        "    return classification"
      ],
      "metadata": {
        "id": "reUYvfDIkOHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def naiveBayesClassifier(trainingSet,testSet):\n",
        "  priors = calculate_prior_probabilities(trainingSet)\n",
        "\n",
        "  for doc in testSet:\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "    print(f\"\\nTest Document: {doc} \\n\")\n",
        "    classified_result = classify_text(doc[0], priors, likelihoods)\n",
        "    print(f\"\\nInferred Class: {classified_result}\")\n",
        "\n",
        "naiveBayesClassifier(trainingSet,testSet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQWStu3giczv",
        "outputId": "1c341cf3-33a4-4f5a-eac1-0fd65a8528db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test Document: ('predictable with no fun', '?') \n",
            "\n",
            "Word: predictable \t wordConditionalProbPos: 0.05 \t wordConditionalProbNeg: 0.05\n",
            "Word: with \t wordConditionalProbPos: 0.047619047619047616 \t wordConditionalProbNeg: 0.047619047619047616\n",
            "Word: no \t wordConditionalProbPos: 0.05 \t wordConditionalProbNeg: 0.05\n",
            "Word: fun \t wordConditionalProbPos: 0.05 \t wordConditionalProbNeg: 0.05\n",
            "\n",
            "docProbPos: 0.4000000000000002\n",
            "docProbNeg: 0.5999999999999999\n",
            "\n",
            "Inferred Class: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Write a Sentiment Analysis function that takes a string as input and identifies its sentiment using the TextBlob library."
      ],
      "metadata": {
        "id": "ToPvxp4jnUwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def sentimentAnalyzer(text):\n",
        "    analysis = TextBlob(text)\n",
        "    print(\"----------------------------------\")\n",
        "    print(f\"String = {text}\\n\" )\n",
        "    if analysis.sentiment[0] > 0.1 :\n",
        "      print(\"Positive Sentiment \")\n",
        "    elif analysis.sentiment[0] < -0.1 :\n",
        "      print(\"Negative Sentiment \")\n",
        "    else :\n",
        "      print(\"Neutral Sentiment\")\n",
        "\n",
        "    print(analysis.sentiment)"
      ],
      "metadata": {
        "id": "CJHurLW8nfcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimentAnalyzer(\"NLP is cool\")\n",
        "sentimentAnalyzer(\"NLP is cool and useful\")\n",
        "sentimentAnalyzer(\"NLP is hard\")\n",
        "sentimentAnalyzer(\"NLP is hard and useless\")\n",
        "sentimentAnalyzer(\"NLP stands for Natural Language Processing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9djEBLVYnq3B",
        "outputId": "9ac1feea-c7ae-429f-d7be-c54b5dde1c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "String = NLP is cool\n",
            "\n",
            "Positive Sentiment \n",
            "Sentiment(polarity=0.35, subjectivity=0.65)\n",
            "----------------------------------\n",
            "String = NLP is cool and useful\n",
            "\n",
            "Positive Sentiment \n",
            "Sentiment(polarity=0.32499999999999996, subjectivity=0.325)\n",
            "----------------------------------\n",
            "String = NLP is hard\n",
            "\n",
            "Negative Sentiment \n",
            "Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666)\n",
            "----------------------------------\n",
            "String = NLP is hard and useless\n",
            "\n",
            "Negative Sentiment \n",
            "Sentiment(polarity=-0.39583333333333337, subjectivity=0.37083333333333335)\n",
            "----------------------------------\n",
            "String = NLP stands for Natural Language Processing\n",
            "\n",
            "Neutral Sentiment\n",
            "Sentiment(polarity=0.1, subjectivity=0.4)\n"
          ]
        }
      ]
    }
  ]
}
