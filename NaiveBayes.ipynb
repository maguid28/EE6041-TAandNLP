{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIkk8E6Q6luVfWz6cFiO8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maguid28/EE6041-TAandNLP/blob/main/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NB Example where I provide the training data"
      ],
      "metadata": {
        "id": "vCxJ1AHeY0O3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO919EcfR3r8"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    ('I love this product', 1),\n",
        "    ('This is an amazing movie', 1),\n",
        "    ('I feel great this morning', 1),\n",
        "    ('I am so excited about the concert', 1),\n",
        "    ('He is my best friend', 1),\n",
        "    ('I do not like this car', 0),\n",
        "    ('This view is horrible', 0),\n",
        "    ('I feel tired this morning', 0),\n",
        "    ('I am not looking forward to the concert', 0),\n",
        "    ('He is my enemy', 0)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example where I"
      ],
      "metadata": {
        "id": "TJI1XOF0Y8UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rarfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baTmdj6GUJoY",
        "outputId": "82a1b1c7-e7b8-4459-9185-48d1cf71a7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdu2MMThTr4_",
        "outputId": "de39224d-cc7a-4e53-e025-22767ddbfd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 10:30:46--  http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
            "Resolving www.cs.uic.edu (www.cs.uic.edu)... 131.193.32.29\n",
            "Connecting to www.cs.uic.edu (www.cs.uic.edu)|131.193.32.29|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar [following]\n",
            "--2023-11-24 10:30:46--  https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
            "Connecting to www.cs.uic.edu (www.cs.uic.edu)|131.193.32.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24020 (23K)\n",
            "Saving to: ‘opinion-lexicon-English.rar’\n",
            "\n",
            "opinion-lexicon-Eng 100%[===================>]  23.46K   129KB/s    in 0.2s    \n",
            "\n",
            "2023-11-24 10:30:47 (129 KB/s) - ‘opinion-lexicon-English.rar’ saved [24020/24020]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rarfile\n",
        "\n",
        "def extract_rar(rar_path, output_path):\n",
        "    with rarfile.RarFile(rar_path) as opened_rar:\n",
        "        opened_rar.extractall(path=output_path)\n",
        "\n",
        "rar_path = 'opinion-lexicon-English.rar'\n",
        "output_path = '..'\n",
        "extract_rar(rar_path, output_path)\n"
      ],
      "metadata": {
        "id": "6k26JHmSUO0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "positive_words_file_path = '/opinion-lexicon-English/positive-words.txt'\n",
        "negative_words_file_path = '/opinion-lexicon-English/negative-words.txt'\n",
        "\n",
        "positive_words = {}\n",
        "negative_words = {}\n",
        "\n",
        "def load_words(file_path, words_dict):\n",
        "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
        "        for line in file:\n",
        "            if line.startswith(';') or not line.strip():\n",
        "                continue\n",
        "            word = line.strip()\n",
        "            words_dict[word] = True\n"
      ],
      "metadata": {
        "id": "B-_BMv5cTklQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_words(positive_words_file_path, positive_words)\n",
        "load_words(negative_words_file_path, negative_words)\n",
        "\n",
        "len_positive = len(positive_words)\n",
        "len_negative = len(negative_words)\n",
        "len_positive, len_negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMrUxQxQ9qvd",
        "outputId": "e4b546f4-2e04-4a5a-f568-1827afe1f657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2006, 4783)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MultinomialNB"
      ],
      "metadata": {
        "id": "fnLTpcnNjMEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "mock_dataset = [\n",
        "    (\"love happy joy\", 1),\n",
        "    (\"excited fun brilliant\", 1),\n",
        "    (\"hate terrible awful\", 0),\n",
        "    (\"bad horrible disappointing\", 0),\n",
        "    (\"wonderful great excellent\", 1),\n",
        "    (\"disaster pathetic sad\", 0),\n",
        "    (\"enjoy pleasant nice\", 1),\n",
        "    (\"nasty cruel filthy\", 0),\n",
        "    (\"beautiful attractive lovely\", 1),\n",
        "    (\"ugly disgusting gross\", 0)\n",
        "]\n",
        "\n",
        "#Convert text into feature vectors\n",
        "def create_features_from_lexicon(texts, positive_lexicon, negative_lexicon):\n",
        "    features = np.zeros((len(texts), 2))\n",
        "    for i, text in enumerate(texts):\n",
        "        words = text.lower().split()\n",
        "        features[i, 0] = sum(word in positive_lexicon for word in words)\n",
        "        features[i, 1] = sum(word in negative_lexicon for word in words)\n",
        "    return features\n",
        "\n",
        "texts = [text for text, label in mock_dataset]\n",
        "labels = [label for text, label in mock_dataset]\n",
        "features = create_features_from_lexicon(texts, positive_words, negative_words)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qha70S9WRYK",
        "outputId": "c28b6fd6-78e5-4b28-90af-3f8f777a27e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#positive_words = ['love', 'happy', 'joy', 'excited', 'fun', 'brilliant', 'wonderful', 'great', 'excellent', 'enjoy', 'pleasant', 'nice', 'beautiful', 'attractive', 'lovely']\n",
        "#negative_words = ['hate', 'terrible', 'awful', 'bad', 'horrible', 'disappointing', 'disaster', 'pathetic', 'sad', 'nasty', 'cruel', 'filthy', 'ugly', 'disgusting', 'gross']\n",
        "\n",
        "sample_texts = ['I feel happy this morning', 'I hate the rain']\n",
        "\n",
        "sample_features = create_features_from_lexicon(sample_texts, positive_words, negative_words)\n",
        "\n",
        "predictions = classifier.predict(sample_features)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inZmGgtavQlJ",
        "outputId": "de0a6a7b-40b2-4c4c-88bc-b2d11a86b1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#positive_words = ['love', 'happy', 'joy', 'excited', 'fun', 'brilliant', 'wonderful', 'great', 'excellent', 'enjoy', 'pleasant', 'nice', 'beautiful', 'attractive', 'lovely']\n",
        "#negative_words = ['hate', 'terrible', 'awful', 'bad', 'horrible', 'disappointing', 'disaster', 'pathetic', 'sad', 'nasty', 'cruel', 'filthy', 'ugly', 'disgusting', 'gross']\n",
        "\n",
        "sample_texts = ['I feel happy this morning', 'I hate the rain', 'I dont know about that']\n",
        "\n",
        "sample_features = create_features_from_lexicon(sample_texts, positive_words, negative_words)\n",
        "\n",
        "predictions = classifier.predict(sample_features)\n",
        "\n",
        "for text, prediction in zip(sample_texts, predictions):\n",
        "    predict_text = \"\"\n",
        "    if prediction == 0 :\n",
        "      predict_text = \"Negative\"\n",
        "    else :\n",
        "      predict_text = \"Positive\"\n",
        "    print(f\"Text: '{text}' - Prediction: {predict_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty5nn2jbVeH-",
        "outputId": "ce5183c4-3b7d-4e93-e91b-3cde502b1f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'I feel happy this morning' - Prediction: Positive\n",
            "Text: 'I hate the rain' - Prediction: Negative\n",
            "Text: 'I dont know about that' - Prediction: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mock dataset"
      ],
      "metadata": {
        "id": "sgZpsYXlq6ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mock_dataset = [\n",
        "    (\"love happy love joy\", 1),\n",
        "    (\"excited fun brilliant\", 1),\n",
        "    (\"hate terrible awful\", 0),\n",
        "    (\"bad horrible disappointing\", 0),\n",
        "    (\"wonderful great excellent\", 1),\n",
        "    (\"disaster pathetic sad\", 0),\n",
        "    (\"enjoy pleasant nice\", 1),\n",
        "    (\"nasty cruel filthy\", 0),\n",
        "    (\"beautiful attractive lovely\", 1),\n",
        "    (\"ugly disgusting gross\", 0)\n",
        "]"
      ],
      "metadata": {
        "id": "Oqmr5MTdbTdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of Words\n"
      ],
      "metadata": {
        "id": "bk2s9oVrfSzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bag_of_words(sentences):\n",
        "    bag_of_words = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in bag_of_words:\n",
        "                bag_of_words[word] = 1\n",
        "            else:\n",
        "                bag_of_words[word] += 1\n",
        "\n",
        "    return bag_of_words"
      ],
      "metadata": {
        "id": "mtbgtYviriqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Never jump over the lazy dog quickly\",\n",
        "    \"The quick fox is quick\"\n",
        "]\n",
        "\n",
        "bag_of_words = create_bag_of_words(sentences)\n",
        "print(bag_of_words)"
      ],
      "metadata": {
        "id": "tzPQlnvornem",
        "outputId": "64621406-a0de-4337-c094-f8f4a1c6a60e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 4, 'quick': 3, 'brown': 1, 'fox': 2, 'jumps': 1, 'over': 2, 'lazy': 2, 'dog': 2, 'never': 1, 'jump': 1, 'quickly': 1, 'is': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag Of Words with min frequency param\n",
        "list to dictionary"
      ],
      "metadata": {
        "id": "NhYVzYhkwbom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bag_of_words_min_freq(dataset, min_frequency=1):\n",
        "    word_frequencies = {}\n",
        "\n",
        "    for sample, _ in dataset:\n",
        "        for word in sample.lower().split():\n",
        "            if word in word_frequencies:\n",
        "                word_frequencies[word] += 1\n",
        "            else:\n",
        "                word_frequencies[word] = 1\n",
        "\n",
        "    #Filter on min_frequency\n",
        "    bag_of_words = {word: count for word, count in word_frequencies.items() if count >= min_frequency}\n",
        "\n",
        "    return bag_of_words\n",
        "\n",
        "bag_of_words = create_bag_of_words_min_freq(mock_dataset)\n",
        "print(bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB1VBtK2u9PQ",
        "outputId": "95e15dff-8444-4909-b6d8-f0ff5a9ffa1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 2, 'happy': 1, 'joy': 1, 'excited': 1, 'fun': 1, 'brilliant': 1, 'hate': 1, 'terrible': 1, 'awful': 1, 'bad': 1, 'horrible': 1, 'disappointing': 1, 'wonderful': 1, 'great': 1, 'excellent': 1, 'disaster': 1, 'pathetic': 1, 'sad': 1, 'enjoy': 1, 'pleasant': 1, 'nice': 1, 'nasty': 1, 'cruel': 1, 'filthy': 1, 'beautiful': 1, 'attractive': 1, 'lovely': 1, 'ugly': 1, 'disgusting': 1, 'gross': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag Of Words with dict input param"
      ],
      "metadata": {
        "id": "vIM3OC1uPIRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bow_dict_input(dataset):\n",
        "    all_words = []\n",
        "    for sample in dataset.keys():\n",
        "        all_words.extend(sample.lower().split())\n",
        "\n",
        "    word_frequencies = {}\n",
        "    for word in all_words:\n",
        "        if word in word_frequencies:\n",
        "            word_frequencies[word] += 1\n",
        "        else:\n",
        "            word_frequencies[word] = 1\n",
        "\n",
        "    bag_of_words = {word: count for word, count in word_frequencies.items()}\n",
        "\n",
        "    return bag_of_words\n",
        "\n",
        "# Example usage with a mock dataset as a dictionary\n",
        "mock_dataset_dict = {'Sample text data': 1, 'Another sample text': 0}\n",
        "bag_of_words = create_bow_dict_input(mock_dataset_dict)\n",
        "print(bag_of_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7nC9D0b6Yer",
        "outputId": "05be7359-3a4d-4997-9ddf-5323be72ac35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sample': 2, 'text': 2, 'data': 1, 'another': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of words"
      ],
      "metadata": {
        "id": "dRVhfsJZ6W5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bow(dataset):\n",
        "    all_words = []\n",
        "    for sample, _ in dataset:\n",
        "        all_words.extend(sample.lower().split())\n",
        "\n",
        "    word_frequencies = {}\n",
        "    for word in all_words:\n",
        "        if word in word_frequencies:\n",
        "            word_frequencies[word] += 1\n",
        "        else:\n",
        "            word_frequencies[word] = 1\n",
        "\n",
        "    bag_of_words = {word: count for word, count in word_frequencies.items()}\n",
        "\n",
        "    return bag_of_words\n",
        "\n",
        "bag_of_words = create_bow(mock_dataset)\n",
        "print(bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBrKKBoez97_",
        "outputId": "6a60905c-9b93-45f8-e0ef-1fc0bf61d453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 2, 'happy': 1, 'joy': 1, 'excited': 1, 'fun': 1, 'brilliant': 1, 'hate': 1, 'terrible': 1, 'awful': 1, 'bad': 1, 'horrible': 1, 'disappointing': 1, 'wonderful': 1, 'great': 1, 'excellent': 1, 'disaster': 1, 'pathetic': 1, 'sad': 1, 'enjoy': 1, 'pleasant': 1, 'nice': 1, 'nasty': 1, 'cruel': 1, 'filthy': 1, 'beautiful': 1, 'attractive': 1, 'lovely': 1, 'ugly': 1, 'disgusting': 1, 'gross': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prior Probabilities Calculation"
      ],
      "metadata": {
        "id": "t8fmB8x5aEZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_prior_probabilities(dataset):\n",
        "    total_samples = len(dataset)\n",
        "    positive_samples = sum(label for _, label in dataset)\n",
        "    negative_samples = total_samples - positive_samples\n",
        "\n",
        "    prior_probability_positive = positive_samples / total_samples\n",
        "    prior_probability_negative = negative_samples / total_samples\n",
        "\n",
        "    return prior_probability_positive, prior_probability_negative"
      ],
      "metadata": {
        "id": "Spjx-u9Vq-p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Likelihood"
      ],
      "metadata": {
        "id": "yiW9ALl9cb8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_likelihoods_with_laplace_smoothing(dataset, bag_of_words):\n",
        "    positive_samples = [text for text, label in dataset if label == 1]\n",
        "    negative_samples = [text for text, label in dataset if label == 0]\n",
        "\n",
        "    # Initialize word count dictionaries\n",
        "    positive_word_counts = {word: 0 for word in bag_of_words}\n",
        "    negative_word_counts = {word: 0 for word in bag_of_words}\n",
        "\n",
        "    for text in positive_samples:\n",
        "        for word in text.split():\n",
        "            if word in bag_of_words:\n",
        "                positive_word_counts[word] += 1\n",
        "\n",
        "    for text in negative_samples:\n",
        "        for word in text.split():\n",
        "            if word in bag_of_words:\n",
        "                negative_word_counts[word] += 1\n",
        "\n",
        "    vocab_size = len(bag_of_words)\n",
        "\n",
        "    laplace_smoothing_factor = 1\n",
        "    positive_total_words = sum(positive_word_counts.values())\n",
        "    negative_total_words = sum(negative_word_counts.values())\n",
        "\n",
        "    positive_likelihoods = {}\n",
        "    for word in bag_of_words:\n",
        "        word_count = positive_word_counts.get(word, 0)\n",
        "        likelihood = (word_count + laplace_smoothing_factor) / (positive_total_words + vocab_size * laplace_smoothing_factor)\n",
        "        positive_likelihoods[word] = likelihood\n",
        "\n",
        "    negative_likelihoods = {}\n",
        "    for word in bag_of_words:\n",
        "        word_count = negative_word_counts.get(word, 0)\n",
        "        likelihood = (word_count + laplace_smoothing_factor) / (negative_total_words + vocab_size * laplace_smoothing_factor)\n",
        "        negative_likelihoods[word] = likelihood\n",
        "\n",
        "    return positive_likelihoods, negative_likelihoods"
      ],
      "metadata": {
        "id": "Hi-kykFGrCB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Posterior Prob Calculated"
      ],
      "metadata": {
        "id": "V0qyY1Rde4dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_posterior_probability(new_sample, prior_probabilities, likelihoods):\n",
        "    positive_likelihoods, negative_likelihoods = likelihoods\n",
        "    prior_probability_positive, prior_probability_negative = prior_probabilities\n",
        "\n",
        "    words = new_sample.split()\n",
        "\n",
        "    positive_prob = np.log(prior_probability_positive)\n",
        "    negative_prob = np.log(prior_probability_negative)\n",
        "\n",
        "    for word in words:\n",
        "        positive_prob += np.log(positive_likelihoods.get(word, 1 / (sum(positive_likelihoods.values()) + len(positive_likelihoods))))\n",
        "        negative_prob += np.log(negative_likelihoods.get(word, 1 / (sum(negative_likelihoods.values()) + len(negative_likelihoods))))\n",
        "\n",
        "    positive_prob = np.exp(positive_prob)\n",
        "    negative_prob = np.exp(negative_prob)\n",
        "\n",
        "    total_prob = positive_prob + negative_prob\n",
        "    posterior_probability_positive = positive_prob / total_prob\n",
        "    posterior_probability_negative = negative_prob / total_prob\n",
        "\n",
        "    return posterior_probability_positive, posterior_probability_negative\n",
        "\n",
        "#new_sample = \"pleasant and good\"\n",
        "#posterior_prob = calculate_posterior_probability(new_sample, prior_probabilities, likelihoods)\n",
        "#posterior_probabilities"
      ],
      "metadata": {
        "id": "j5YcYXg5e7-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classify Text as Postive or Negative"
      ],
      "metadata": {
        "id": "WM2QQ58AiWhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(sentence, prior_probabilities, likelihoods):\n",
        "\n",
        "    posterior_probability_positive, posterior_probability_negative = calculate_posterior_probability(sentence, prior_probabilities, likelihoods)\n",
        "\n",
        "    print(f\"String: {sentence}\")\n",
        "    print(f\"posterior_probability_positive: {posterior_probability_positive}\")\n",
        "    print(f\"posterior_probability_negative: {posterior_probability_negative}\")\n",
        "\n",
        "    max_probability = max(posterior_probability_positive, posterior_probability_negative)\n",
        "    if max_probability == posterior_probability_positive and max_probability != posterior_probability_negative:\n",
        "        classification = \"Positive\"\n",
        "        other_probabilities = [posterior_probability_negative]\n",
        "        confidence = max_probability - sum(other_probabilities) / len(other_probabilities)\n",
        "    elif max_probability == posterior_probability_negative and max_probability != posterior_probability_positive:\n",
        "        classification = \"Negative\"\n",
        "        other_probabilities = [posterior_probability_positive]\n",
        "        confidence = max_probability - sum(other_probabilities) / len(other_probabilities)\n",
        "    else:\n",
        "        classification = \"Neutral\"\n",
        "        other_probabilities = [posterior_probability_positive, posterior_probability_negative]\n",
        "        confidence = posterior_probability_positive / posterior_probability_negative\n",
        "\n",
        "    return classification, confidence"
      ],
      "metadata": {
        "id": "gHZM_CqRg6AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = create_bow(mock_dataset)\n",
        "\n",
        "prior_probabilities = calculate_prior_probabilities(mock_dataset)\n",
        "likelihoods = calculate_likelihoods_with_laplace_smoothing(mock_dataset, bag_of_words)"
      ],
      "metadata": {
        "id": "BT63MIM2r4dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sample = \"happy and fun\"\n",
        "classified_result = classify_text(new_sample, prior_probabilities, likelihoods)\n",
        "print(f\"Sentiment: {classified_result[0]}\")\n",
        "print(f\"Confidence: {classified_result[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPkxCcOQyjIf",
        "outputId": "268e43ef-98c5-44d3-bbef-4d8d60852162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String: happy and fun\n",
            "posterior_probability_positive: 0.7928739232576351\n",
            "posterior_probability_negative: 0.20712607674236497\n",
            "Sentiment: Positive\n",
            "Confidence: 0.5857478465152701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sample = \"ugly and fun\"\n",
        "classified_result = classify_text(new_sample, prior_probabilities, likelihoods)\n",
        "print(f\"Sentiment: {classified_result[0]}\")\n",
        "print(f\"Confidence: {classified_result[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOHyboyiyjoo",
        "outputId": "e52e11c7-2502-4cc6-d5d4-931bb9380dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String: ugly and fun\n",
            "posterior_probability_positive: 0.48901231586573285\n",
            "posterior_probability_negative: 0.5109876841342672\n",
            "Sentiment: Negative\n",
            "Confidence: 0.021975368268534357\n"
          ]
        }
      ]
    }
  ]
}